\input src/format

\sec{XGBoost Algorithrm}
\mnote{2024-12-29}

XGBoost \url{https://arxiv.org/abs/1603.02754} is a highly scalable gradient
boosting machine learning library. We will study its basic algorithm and then
the parallel optimization algorithm.

\subsec{Gradient Boosting Tree}

\mnote{Loss}
Let $\bar{x}$ be an $m$-feature input sample and $y$ be the coressponding real
value label.  Given an $N$-length input sequence $(\bar{x}_1, y_1), (\bar{x}_2,
y_2), \ldots, (\bar{x}_N,y_N)$, gradient boosting aims to optimize the function
approximation with an additive manner, i.e., after iteration $ K-1 $, train a
new $ f_{K }$ (usually a tree) such that for predictions
$$
{\hat{y}}_{i,(K)}
= {\hat{y}}_{i} = \phi_K(\bar{x}_i)
= \sum_{k=1}^K f_{k} (\bar{x}_i)
= \left[ \sum_{k=1}^{K-1} f_{k} (\bar{x}_i) \right] + f_K(\bar{x}_i)
$$
minimize the loss of
$$
\eqalignno{
L
&= \left[
    \sum_{i=1}^N l (y_i , \hat{y}_i)
  \right]
+ \Omega(\phi_K)
\cr
&= \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \left[
           \sum_{k=1}^{K-1} f_{k} (\bar{x}_i)
        \right] + f_K(\bar{x}_i)
      \right)
  \right\}
+ \Omega(\phi_K)
\cr
&= \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \hat{y}_{i,{(K-1)}} + f_K(\bar{x}_i)
      \right)
  \right\}
+ \Omega(\phi_K)
}
$$
where $l(\cdot)$ is a convex loss function and $\Omega$ is the regularization on
the number of leaves $T_{\{\cdot\}}$ and squares sum of all leaves weights
$w_{\{\cdot\},j}, \forall j \in\{1, 2, \ldots, T\}$ of all trees, i.e.,
$$
\eqalignno{
\Omega(\phi_K)
&
= \alpha \left( \sum_{k=1}^K T_k
  \right)
+ \gamma \left[
  \sum_{k=1}^K \left( \sum_{j=1}^{T_k} || w_{k,j} ||^2
               \right)
  \right]
\cr
%&
%= \left\{
%    \alpha
%      \left(
%        \sum_{k=1}^{K-1} T_k
%      \right)
%    + \gamma
%      \left[
%        \sum_{k=1}^{K-1}
%          \left( \sum_{j=1}^{T_k} || w_{k,j} ||^2
%          \right)
%      \right]
%  \right\}
%+ \left\{
%    \alpha
%      T_K
%    + \gamma
%        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
%        \right)
%  \right\}
%\cr
&
= \Omega(\phi_{K-1})
+ \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
}
$$

\mnote{Boosting}

The idea of boosting is to do approximation in the function space. This can be
done at each input sample $\bar{x}$ with the second order Taylor expansion
$$
\eqalignno{
  L_{(K)}
&= C_1 +
   \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \hat{y}_{i,{(K-1)}} + f_K(\bar{x}_i)
      \right)
  \right\}
+ \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
\cr
& \approx C_1 +
   \left\{
    \sum_{i=1}^N
      \left[
        l
        \left(
          y_i ,
          \hat{y}_{i,{(K-1)}}
        \right)
        + g_{i,(K-1)} f_K(\bar{x}_i)
        + {1\over2} h_{i,(K-1)} f_K^2(\bar{x}_i)
      \right]
  \right\}
\cr
&
\quad + \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
\cr
& = C_2 +
   \left\{
    \sum_{i=1}^N
      \left[
        g_{i,(K-1)} f_K(\bar{x}_i)
        + {1\over2} h_{i,(K-1)} f_K^2(\bar{x}_i)
      \right]
  \right\}
\cr
&
\quad + \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
}
$$
where $g_i$ and $h_i$ are the first and second order of derivatives of the loss
funciton w.r.t. previous prediction $\hat{y}_{i,(K-1)}$. For a fixed tree
structure $f_K$, we can define $I_j$ to be the set of sample input indexes such
that they are mapped to the leave with index $j$, i.e.,
$$
I_j =
\left\{
  i | q(\bar{x}_i)=j
\right\},\quad \forall j = \{1,2,\ldots,T_K\}
$$
With this, we can rewrite the loss to be optimized as
$$
\eqalignno{
  L
& = C_2 + \alpha T
+ \sum_{j=1}^{T}
 \left\{
   \left( \sum_{i\in I_j} g_i \right) w_j
   +
   {1\over2}
   \left[
     \left(
     \sum_{i\in I_j} h_i
     \right)
     + \gamma
    \right] w_j^2
  \right\}
}
$$
where the iteration $K$ is removed from the equation toward simplicity. Note
that both $g_i$ and $h_i$ are functions over $\bar{x}_{i}$ determined by the
boosting tree essembled in the $K-1$ iteration, so there are constants during
optimization in iteration $K$.

This loss $L$ has minimal value, for a fixed tree structure, as
$$
\eqalignno{
  L^*
& = C_2 + \alpha T
-
 {1\over2}
 \sum_{j=1}^{T}
 {
   \left( \sum_{i\in I_j} g_i \right)^2
   \over
   \left[
     \left(
     \sum_{i\in I_j} h_i
     \right)
     + \gamma
    \right]
  }
}
$$
And in order to greedily split the tree, we can focus one leave $j$ at a time,
followed by going through
all feature dimensions (total $m$ of them), and then, for each dimension $d$,
going through (sorted) sample input values ($\{x_{i,d} | \forall i\}$) to find a split
$I_{j,(d,L)}$ and $I_{j,(d,R)}$, such that the loss reduction is
$$
\eqalignno{
  L^*_{j, reduction}
& = - \alpha
+
  {1 \over 2}
  \left[
    {
      \left( \sum_{i\in I_{j,(d,L)}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j,(d,L)}} h_i
      \right)
      + \gamma
    }
    +
    {
      \left( \sum_{i\in I_{j,(d,R)}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j,(d,R)}} h_i
      \right)
      + \gamma
    }
    -
    {
      \left( \sum_{i\in I_{j}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j}} h_i
      \right)
      + \gamma
    }
  \right]
}
$$


\vfill
\bye
